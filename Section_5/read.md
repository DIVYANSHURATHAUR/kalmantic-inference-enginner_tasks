This repo demonstrates an optimized local inference pipeline using Hugging Face Transformers.
It includes caching, batching simulation, and latency tracking to showcase efficient LLM serving principles.

## Run
pip install -r requirements.txt
python inference_pipeline.py